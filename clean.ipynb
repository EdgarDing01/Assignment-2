{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1296e803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset info:\n",
      "Rows: 11546, Columns: 21\n",
      "\n",
      "Columns and dtypes:\n",
      " - As of Date: object  (missing%: 0.0  unique: 1)\n",
      " - Project ID: object  (missing%: 0.0  unique: 11546)\n",
      " - Project Name: object  (missing%: 0.0  unique: 10158)\n",
      " - WB Region: object  (missing%: 0.0  unique: 9)\n",
      " - Country / Economy: object  (missing%: 0.51  unique: 182)\n",
      " - Country / Economy Lending Group: object  (missing%: 0.0  unique: 4)\n",
      " - Country / Economy FCS Status: object  (missing%: 55.46  unique: 3)\n",
      " - Country / Economy FCS Lending Group: object  (missing%: 0.0  unique: 7)\n",
      " - Practice Group: object  (missing%: 0.0  unique: 6)\n",
      " - Global Practice: object  (missing%: 0.0  unique: 18)\n",
      " - Agreement Type: object  (missing%: 0.01  unique: 10)\n",
      " - Lending Instrument Type: object  (missing%: 0.05  unique: 4)\n",
      " - Approval FY: float64  (missing%: 0.2  unique: 58)\n",
      " - Final Closing FY: float64  (missing%: 32.41  unique: 50)\n",
      " - Evaluation Type: object  (missing%: 0.0  unique: 7)\n",
      " - Outcome: object  (missing%: 1.2  unique: 7)\n",
      " - Quality at Entry: object  (missing%: 24.37  unique: 7)\n",
      " - Quality of Supervision: object  (missing%: 18.61  unique: 7)\n",
      " - Bank Performance: object  (missing%: 28.83  unique: 7)\n",
      " - M&E Quality: object  (missing%: 57.9  unique: 5)\n",
      " - Evaluation FY: int64  (missing%: 0.0  unique: 49)\n",
      "\n",
      "Automatically selected columns to drop (by heuristic):\n",
      " - as_of_date\n",
      " - project_id\n",
      "\n",
      "Dropped 0 duplicate rows.\n",
      "\n",
      "Cleaned dataset saved to: cleaned_world_bank.csv\n",
      "\n",
      "Summary of remaining columns:\n",
      "\n",
      "Summary of cleaning actions taken:\n",
      "- Normalised column names (trimmed, lowercased, spaces replaced with underscores).\n",
      "- Removed 2 columns automatically because they were constant, had >=60% missing values, or appeared to be identifiers/notes/URLs according to heuristics.\n",
      "- Stripped whitespace in text columns and attempted to convert 'year' columns to integers when possible.\n",
      "- Removed duplicate rows.\n",
      "- Saved the cleaned dataset to: cleaned_world_bank.csv\n",
      "\n",
      "No obvious GDP/inflation/unemployment columns were detected by simple name heuristics. You may want to provide or merge a World Bank indicators dataset (e.g., GDP, inflation, unemployment) if this file is a different table (e.g., project ratings).\n",
      "\n",
      "[Download cleaned CSV](/mnt/data/cleaned_world_bank.csv)\n"
     ]
    }
   ],
   "source": [
    "# Loading and cleaning the uploaded CSV for the user\n",
    "# This code will:\n",
    "# 1. Read the CSV file at /mnt/data/ieg_world_bank_project_performance_ratings_10-16-2025.csv\n",
    "# 2. Inspect columns, data types, missingness, and basic stats\n",
    "# 3. Drop obviously non-informative columns (constant columns, columns with mostly missing values, obvious IDs/urls)\n",
    "# 4. Remove duplicate rows and normalize column names (strip, lower)\n",
    "# 5. Output the cleaned dataframe and save it to /mnt/data/cleaned_world_bank.csv\n",
    "# The reasoning for dropped columns will be printed.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "input_path = \"ieg_world_bank_project_performance_ratings_10-16-2025.csv\"\n",
    "df = pd.read_csv(input_path, low_memory=False)\n",
    "\n",
    "# Inspect\n",
    "info = {\n",
    "    \"num_rows\": len(df),\n",
    "    \"num_columns\": df.shape[1],\n",
    "    \"columns\": df.columns.tolist(),\n",
    "    \"dtypes\": df.dtypes.apply(lambda x: str(x)).to_dict(),\n",
    "    \"missing_perc\": (df.isna().mean()*100).round(2).to_dict(),\n",
    "    \"n_unique\": df.nunique(dropna=False).to_dict(),\n",
    "}\n",
    "\n",
    "print(\"Initial dataset info:\")\n",
    "print(f\"Rows: {info['num_rows']}, Columns: {info['num_columns']}\")\n",
    "print(\"\\nColumns and dtypes:\")\n",
    "for c, t in info[\"dtypes\"].items():\n",
    "    print(f\" - {c}: {t}  (missing%: {info['missing_perc'][c]}  unique: {info['n_unique'][c]})\")\n",
    "\n",
    "# Normalize column names: strip and lower for consistent handling, keep mapping to original\n",
    "orig_cols = df.columns.tolist()\n",
    "norm_map = {c: c.strip().lower().replace(\" \", \"_\") for c in orig_cols}\n",
    "df = df.rename(columns=norm_map)\n",
    "\n",
    "# Decide which columns to drop automatically:\n",
    "to_drop = []\n",
    "\n",
    "# 1) drop columns that are constant (single unique non-null value)\n",
    "for col in df.columns:\n",
    "    nunique = df[col].nunique(dropna=True)\n",
    "    if nunique <= 1:\n",
    "        to_drop.append(col)\n",
    "\n",
    "# 2) drop columns with very high missingness (>=60%)\n",
    "high_missing_thresh = 0.60\n",
    "missing_frac = df.isna().mean()\n",
    "for col in df.columns:\n",
    "    if missing_frac[col] >= high_missing_thresh and col not in to_drop:\n",
    "        to_drop.append(col)\n",
    "\n",
    "# 3) drop obvious identifiers / urls / notes that are text heavy and unlikely to be \"economic indicators\"\n",
    "# Heuristics: column names containing \"id\", \"url\", \"link\", \"notes\", \"description\", \"comment\", \"source\", \"code\"\n",
    "identifier_keywords = [\"id\", \"url\", \"link\", \"notes\", \"note\", \"description\", \"comment\", \"source\", \"code\", \"hash\"]\n",
    "for col in df.columns:\n",
    "    low = col.lower()\n",
    "    if any(kw in low for kw in identifier_keywords):\n",
    "        # but avoid dropping 'country' or 'year'\n",
    "        if not any(k in low for k in [\"country\", \"year\"]):\n",
    "            if col not in to_drop:\n",
    "                to_drop.append(col)\n",
    "\n",
    "# 4) drop columns with extremely high cardinality relative to rows (likely unique identifiers)\n",
    "for col in df.columns:\n",
    "    if col in to_drop or col in (\"country\", \"year\"):\n",
    "        continue\n",
    "    if df[col].nunique(dropna=False) / max(1, len(df)) > 0.9:\n",
    "        # skip numeric continuous indicators that may be naturally varied; use name heuristics\n",
    "        if not any(k in col for k in [\"gdp\", \"inflation\", \"unemployment\", \"rate\", \"percent\", \"score\", \"rating\", \"year\"]):\n",
    "            to_drop.append(col)\n",
    "\n",
    "# Ensure we don't accidentally drop the most likely economic columns: keep columns named like country, year, gdp, inflation, unemployment\n",
    "protected_keywords = [\"country\", \"year\", \"gdp\", \"inflation\", \"unemployment\", \"unemployment_rate\", \"population\", \"exchange\", \"rate\"]\n",
    "to_drop = [c for c in to_drop if not any(pk in c for pk in protected_keywords)]\n",
    "\n",
    "print(\"\\nAutomatically selected columns to drop (by heuristic):\")\n",
    "for c in to_drop:\n",
    "    print(\" -\", c)\n",
    "\n",
    "# Apply drops\n",
    "df_clean = df.drop(columns=to_drop, errors=\"ignore\").copy()\n",
    "\n",
    "# Additional cleaning: strip whitespace for object columns, convert year-like to int if possible\n",
    "for col in df_clean.select_dtypes(include=[\"object\"]).columns:\n",
    "    df_clean[col] = df_clean[col].astype(str).str.strip().replace({\"nan\": pd.NA})\n",
    "\n",
    "# Try convert 'year' to integer if present\n",
    "year_candidates = [c for c in df_clean.columns if \"year\" in c]\n",
    "for yc in year_candidates:\n",
    "    try:\n",
    "        df_clean[yc] = pd.to_numeric(df_clean[yc], errors=\"coerce\").astype(\"Int64\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Drop duplicate rows\n",
    "before_dup = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "after_dup = len(df_clean)\n",
    "\n",
    "print(f\"\\nDropped {before_dup - after_dup} duplicate rows.\")\n",
    "\n",
    "# Save cleaned file\n",
    "output_path = \"cleaned_world_bank.csv\"\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nCleaned dataset saved to: {output_path}\")\n",
    "\n",
    "# Show top rows for user to inspect (first 50 rows to respect space)\n",
    "display_df = df_clean.head(50)\n",
    "\n",
    "# Also provide a compact summary table of remaining columns\n",
    "summary = pd.DataFrame({\n",
    "    \"column\": df_clean.columns,\n",
    "    \"dtype\": [str(t) for t in df_clean.dtypes],\n",
    "    \"missing_pct\": (df_clean.isna().mean()*100).round(2).values,\n",
    "    \"n_unique\": df_clean.nunique(dropna=False).values\n",
    "})\n",
    "print(\"\\nSummary of remaining columns:\")\n",
    "\n",
    "# Print a final text block for the user explaining the actions taken\n",
    "print(\"\\nSummary of cleaning actions taken:\")\n",
    "print(\"- Normalised column names (trimmed, lowercased, spaces replaced with underscores).\")\n",
    "print(f\"- Removed {len(to_drop)} columns automatically because they were constant, had >={int(high_missing_thresh*100)}% missing values, or appeared to be identifiers/notes/URLs according to heuristics.\")\n",
    "print(\"- Stripped whitespace in text columns and attempted to convert 'year' columns to integers when possible.\")\n",
    "print(\"- Removed duplicate rows.\")\n",
    "print(f\"- Saved the cleaned dataset to: {output_path}\")\n",
    "\n",
    "# If the dataframe contains obvious economic indicator columns, list them as likely usable attributes\n",
    "likely_indicators = [c for c in df_clean.columns if any(k in c for k in [\"gdp\",\"inflation\",\"unemployment\",\"rate\",\"percent\",\"score\",\"rating\",\"population\",\"gni\",\"income\"])]\n",
    "if likely_indicators:\n",
    "    print(\"\\nLikely economic indicator columns detected (based on name heuristics):\")\n",
    "    for c in likely_indicators:\n",
    "        print(\" -\", c)\n",
    "else:\n",
    "    print(\"\\nNo obvious GDP/inflation/unemployment columns were detected by simple name heuristics. You may want to provide or merge a World Bank indicators dataset (e.g., GDP, inflation, unemployment) if this file is a different table (e.g., project ratings).\")\n",
    "\n",
    "# Provide download link\n",
    "print(f\"\\n[Download cleaned CSV](/mnt/data/cleaned_world_bank.csv)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
